{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIZfgdzXciMg"
   },
   "source": [
    "# This file runs on ***Google Colab***\n",
    "## Before running this file, the Data Source file needs to be placed at the same level as this file\n",
    "\n",
    "### Data file name: PySpark_DataFile/hotel_reviews_for_PySpark_preprocessing.zip/hotel_reviews_for_PySpark_preprocessing.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CarnJEuzwP1B",
    "outputId": "a37667ae-ec9b-4554-c0a7-101833c103ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0% [Working]\r",
      "            \r",
      "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "\r",
      "0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.or\r",
      "                                                                               \r",
      "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
      "\r",
      "0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\r",
      "0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r",
      "                                                                               \r",
      "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "\r",
      "0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r",
      "                                                                               \r",
      "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
      "\r",
      "0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Conn\r",
      "                                                                               \r",
      "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
      "\r",
      "                                                                               \r",
      "Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
      "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
      "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
      "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
      "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
      "Reading package lists... Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.0.1'\n",
    "spark_version = 'spark-3.0.1'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lZilGKyDwUV_"
   },
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Cleaning_Data_for_PySpark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lOu9ay3cwZX9",
    "outputId": "e9c6f80c-e625-49db-cd01-13d2d4b1be59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|              Review|Reviewer_Score|\n",
      "+--------------------+--------------+\n",
      "| I am so angry th...|           2.9|\n",
      "| No real complain...|           7.5|\n",
      "| Rooms are nice b...|           7.1|\n",
      "| My room was dirt...|           3.8|\n",
      "| You When I booke...|           6.7|\n",
      "| Backyard of the ...|           6.7|\n",
      "| Cleaner did not ...|           4.6|\n",
      "| Apart from the p...|          10.0|\n",
      "| Even though the ...|           6.5|\n",
      "| The aircondition...|           7.9|\n",
      "| Nothing all grea...|          10.0|\n",
      "| 6 30 AM started ...|           5.8|\n",
      "| The floor in my ...|           4.6|\n",
      "| This hotel is be...|           9.2|\n",
      "| The staff in the...|           8.8|\n",
      "| This hotel is aw...|          10.0|\n",
      "| Very steep steps...|           6.3|\n",
      "| We did not like ...|           7.5|\n",
      "| Public areas are...|           7.1|\n",
      "| We had issues wi...|           7.5|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in data \n",
    "from pyspark import SparkFiles\n",
    "file_path = \"hotel_reviews_for_PySpark_preprocessing.csv\"\n",
    "spark.sparkContext.addFile(file_path)\n",
    "df = spark.read.csv(SparkFiles.get(\"hotel_reviews_for_PySpark_preprocessing.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for Null, nan and empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sx5ryO5Bwe94"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mYmZ5Fr9I5sb",
    "outputId": "fa81e9b8-da6f-4cde-d914-4c17022e3971"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the df for empty strings\n",
    "df.filter(df[\"Review\"] == \"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "max_XgwtI5zc",
    "outputId": "77973d2a-c4ab-466f-a767-81544873ed3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the df for null values\n",
    "df.filter(df[\"Review\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2R5pIsDNI3nm",
    "outputId": "053c79a8-bb52-46a6-f801-33c387e8f87d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the df for nan values\n",
    "df.filter(isnan(df[\"Review\"])).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qZi4FNbBJJ5M"
   },
   "outputs": [],
   "source": [
    "# Retaining only NotNull values\n",
    "df = df.where(col(\"Review\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jtjt_qLiJKAJ",
    "outputId": "ac5c387f-06a4-47b2-d8e7-a2e485d4cc48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking again for any null values\n",
    "df.filter(df[\"Review\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Yn9OAhWSJKNt"
   },
   "outputs": [],
   "source": [
    "# Exporting cleaned data as CSV\n",
    "df.toPandas().to_csv('data_ready_for_PySpark_NLP.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "s9Y6f72xxQ9p"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Step0a_Using_PySpark_to_clean_all_Null.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
